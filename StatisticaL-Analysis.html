<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Statistical Analysis</title>

<script src="site_libs/header-attrs-2.16/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/flatly.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/navigation-1.1/codefolding.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<script src="site_libs/kePrint-0.0.1/kePrint.js"></script>
<link href="site_libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Home</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="Data-exploration.html">Exploratory statistics</a>
</li>
<li>
  <a href="StatisticaL-Analysis.html">Descriptive statistics</a>
</li>
<li>
  <a href="shiny_edited_copy.html">Diabetes Predictor</a>
</li>
<li>
  <a href="project_report.html">Project Report</a>
</li>
<li>
  <a href="https://youtu.be/iG7VMp0-PUs">
    <span class="fa fa-video fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="https://github.com/Tristaaazjy/jz3571_project.github.io">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">

<div class="btn-group pull-right float-right">
<button type="button" class="btn btn-default btn-xs btn-secondary btn-sm dropdown-toggle" data-toggle="dropdown" data-bs-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu dropdown-menu-right" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>



<h1 class="title toc-ignore">Statistical Analysis</h1>

</div>


<div id="model-selection" class="section level2">
<h2>Model Selection</h2>
<p>Since the outcome of diabetes was a binary variable in our dataset(1
= disease, 0 = no disease), we used the Logistic model to obtain the
primary model which includes every diagnostic measurement in the model
to explore the relationship between these measurements and diabetes. A
logistic model models the probability of an event occurring by letting
the log odds of the event be a linear combination of one or more
independent variables. The analysis uses binary variable regression and
models the probability of diabetes occurring with all the variables.
Since the full model includes every measurement and does not perform
statistical analysis and comparisons with other models to see its
predictive ability, therefore it needs to be further analyzed and
improved to get prediction optimization.</p>
<div id="full-model" class="section level3">
<h3>Full model</h3>
<pre class="r"><code>fullmodel &lt;- glm(Outcome ~. ,data = diabetes)
summary(fullmodel)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Outcome ~ ., data = diabetes)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -1.01348  -0.29513  -0.09541   0.32112   1.24160  
## 
## Coefficients:
##                            Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)              -0.8538943  0.0854850  -9.989  &lt; 2e-16 ***
## Pregnancies               0.0205919  0.0051300   4.014 6.56e-05 ***
## Glucose                   0.0059203  0.0005151  11.493  &lt; 2e-16 ***
## BloodPressure            -0.0023319  0.0008116  -2.873  0.00418 ** 
## SkinThickness             0.0001545  0.0011122   0.139  0.88954    
## Insulin                  -0.0001805  0.0001498  -1.205  0.22857    
## BMI                       0.0132440  0.0020878   6.344 3.85e-10 ***
## DiabetesPedigreeFunction  0.1472374  0.0450539   3.268  0.00113 ** 
## Age                       0.0026214  0.0015486   1.693  0.09092 .  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for gaussian family taken to be 0.1601684)
## 
##     Null deviance: 174.48  on 767  degrees of freedom
## Residual deviance: 121.57  on 759  degrees of freedom
## AIC: 783.82
## 
## Number of Fisher Scoring iterations: 2</code></pre>
<p>The full model result shows <code>Pregnancies</code>,
<code>Glucose</code>, <code>BloodPressure</code>, <code>BMI</code>, and
<code>DiabetesPredigreeFunction</code> are significantly related to
diabetes odds, while <code>SkinThickness</code>, <code>Insulin</code>
and <code>Age</code> are not significantly related to diabetes odds. It
has an AIC around 783. Due to the non-significant variables included in
the model and high AIC, we need to adjust our model.</p>
</div>
<div id="stepwise-regression" class="section level3">
<h3>Stepwise regression</h3>
<p>The other model was fitted using stepwise regression that is a
step-by-step iterative construction of a regression model involving the
selection of independent variables. It adds or removes potential
explanatory variables one by one and test s for statistical significance
after each iteration. The stepwise regression generates six predictors
of statistical significance including <code>Pregnancies</code>,
<code>Glucose</code>, <code>BloodPressure</code>, <code>BMI</code>, and
<code>DiabetesPredigreeFunction</code>, and <code>Age</code>. The final
model eliminating<code>SkinThickness</code> and <code>Insulin</code> has
a smaller AIC.</p>
<pre class="r"><code>stepmodel &lt;- stepAIC(fullmodel,trace = F)
stepmodel$anova</code></pre>
<pre><code>## Stepwise Model Path 
## Analysis of Deviance Table
## 
## Initial Model:
## Outcome ~ Pregnancies + Glucose + BloodPressure + SkinThickness + 
##     Insulin + BMI + DiabetesPedigreeFunction + Age
## 
## Final Model:
## Outcome ~ Pregnancies + Glucose + BloodPressure + BMI + DiabetesPedigreeFunction + 
##     Age
## 
## 
##              Step Df   Deviance Resid. Df Resid. Dev      AIC
## 1                                     759   121.5678 783.8218
## 2 - SkinThickness  1 0.00309149       760   121.5709 781.8413
## 3       - Insulin  1 0.25258410       761   121.8235 781.4353</code></pre>
<pre class="r"><code>summary(stepmodel)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Outcome ~ Pregnancies + Glucose + BloodPressure + 
##     BMI + DiabetesPedigreeFunction + Age, data = diabetes)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -1.10046  -0.29833  -0.09648   0.31272   1.23210  
## 
## Coefficients:
##                            Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)              -0.8362991  0.0843169  -9.919  &lt; 2e-16 ***
## Pregnancies               0.0209264  0.0051218   4.086 4.86e-05 ***
## Glucose                   0.0057091  0.0004832  11.815  &lt; 2e-16 ***
## BloodPressure            -0.0023572  0.0008018  -2.940  0.00338 ** 
## BMI                       0.0130807  0.0019634   6.662 5.17e-11 ***
## DiabetesPedigreeFunction  0.1403017  0.0443929   3.160  0.00164 ** 
## Age                       0.0027917  0.0015323   1.822  0.06886 .  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for gaussian family taken to be 0.1600834)
## 
##     Null deviance: 174.48  on 767  degrees of freedom
## Residual deviance: 121.82  on 761  degrees of freedom
## AIC: 781.44
## 
## Number of Fisher Scoring iterations: 2</code></pre>
</div>
<div id="best-subset-regression" class="section level3">
<h3>Best subset regression</h3>
<p>While stepwise regression select variables sequentially, the best
subsets approach aims to find out the best fit model from all possible
subset models. Best subset regression selects the best model from all
possible subsets according to some goodness-of-fit criteria. The best
model it generated includes seven predictors: ’<code>Pregnancies</code>,
<code>Glucose</code>, <code>BloodPressure</code>, <code>BMI</code>, and
<code>DiabetesPredigreeFunction</code>, and <code>Age</code>, and
<code>Insulin</code>.</p>
<pre class="r"><code>subsetmodel&lt;-glm(Outcome ~ Pregnancies + Glucose + BloodPressure + BMI + DiabetesPedigreeFunction + Age+Insulin,data=diabetes)
summary(subsetmodel)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Outcome ~ Pregnancies + Glucose + BloodPressure + 
##     BMI + DiabetesPedigreeFunction + Age + Insulin, data = diabetes)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -1.01707  -0.29614  -0.09656   0.32073   1.24183  
## 
## Coefficients:
##                            Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)              -0.8537906  0.0854265  -9.994  &lt; 2e-16 ***
## Pregnancies               0.0205939  0.0051266   4.017 6.48e-05 ***
## Glucose                   0.0059092  0.0005086  11.619  &lt; 2e-16 ***
## BloodPressure            -0.0023152  0.0008022  -2.886  0.00401 ** 
## BMI                       0.0133382  0.0019733   6.759 2.76e-11 ***
## DiabetesPedigreeFunction  0.1478835  0.0447843   3.302  0.00100 ** 
## Age                       0.0025991  0.0015393   1.688  0.09173 .  
## Insulin                  -0.0001721  0.0001370  -1.257  0.20929    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for gaussian family taken to be 0.1599617)
## 
##     Null deviance: 174.48  on 767  degrees of freedom
## Residual deviance: 121.57  on 760  degrees of freedom
## AIC: 781.84
## 
## Number of Fisher Scoring iterations: 2</code></pre>
</div>
</div>
<div id="compare-models" class="section level2">
<h2>Compare models</h2>
<div id="cross-validation" class="section level3">
<h3>Cross validation</h3>
<p>To compare across three models, we use a cross validation approach
that splits the data into two sets: one set is used to train the model
and the remaining set is used to test the model. We split so that 80% is
used for training a regression model and 20% is used to evaluate the
model performance.</p>
<p>The process works as follow:</p>
<ul>
<li>Build (train) the model on the training data set</li>
<li>Apply the model to the test data set to predict the outcome of new
unseen observations</li>
<li>Quantify the prediction error as the mean squared difference between
the observed and the predicted outcome values.</li>
</ul>
<p>The results are shown in following table.</p>
<pre class="r"><code>criterion &lt;- c(&quot;AIC&quot;, &quot;AUC&quot;, &quot;RMSE&quot;, &quot;MAE&quot;,&quot;Prediction Accuracy&quot;)
full_val &lt;- c(783.82,0.7135,0.4249,0.3468,0.7451)
step_val &lt;- c(781.44,0.7107,0.4249,0.3467,0.7451)
sub_val &lt;- c(781.84,0.7135,0.4239,0.3461,0.7451)
variable_data &lt;- data.frame(criterion,full_val,step_val,sub_val)
kable(variable_data,caption = &quot;Table 1.Model Comparison&quot;, col.names = c(&quot;Criterion&quot;,&quot;Full Model&quot;,&quot;Stepwise model&quot;,&quot;Subset model&quot;))%&gt;% kable_styling(latex_option = c(&quot;hold_position&quot;), position = &quot;left&quot;)</code></pre>
<table class="table" style>
<caption>
Table 1.Model Comparison
</caption>
<thead>
<tr>
<th style="text-align:left;">
Criterion
</th>
<th style="text-align:right;">
Full Model
</th>
<th style="text-align:right;">
Stepwise model
</th>
<th style="text-align:right;">
Subset model
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
AIC
</td>
<td style="text-align:right;">
783.8200
</td>
<td style="text-align:right;">
781.4400
</td>
<td style="text-align:right;">
781.8400
</td>
</tr>
<tr>
<td style="text-align:left;">
AUC
</td>
<td style="text-align:right;">
0.7135
</td>
<td style="text-align:right;">
0.7107
</td>
<td style="text-align:right;">
0.7135
</td>
</tr>
<tr>
<td style="text-align:left;">
RMSE
</td>
<td style="text-align:right;">
0.4249
</td>
<td style="text-align:right;">
0.4249
</td>
<td style="text-align:right;">
0.4239
</td>
</tr>
<tr>
<td style="text-align:left;">
MAE
</td>
<td style="text-align:right;">
0.3468
</td>
<td style="text-align:right;">
0.3467
</td>
<td style="text-align:right;">
0.3461
</td>
</tr>
<tr>
<td style="text-align:left;">
Prediction Accuracy
</td>
<td style="text-align:right;">
0.7451
</td>
<td style="text-align:right;">
0.7451
</td>
<td style="text-align:right;">
0.7451
</td>
</tr>
</tbody>
</table>
</div>
<div id="root-mean-squared-error-rmse" class="section level3">
<h3>Root Mean Squared Error (RMSE)</h3>
<p>As the name suggests it is the square root of the averaged squared
difference between the actual value and the predicted value of the
target variable. It gives the average prediction error made by the
model, thus decreasing the RMSE value increases the accuracy of the
mode.</p>
</div>
<div id="mean-absolute-error-mae" class="section level3">
<h3>Mean Absolute Error (MAE)</h3>
<p>This metric gives the absolute difference between the actual values
and the values predicted by the model for the target variable. If the
value of the outliers does not have much to do with the accuracy of the
model, then MAE can be used to evaluate the performance of the model.
Its value must be less in order to make better models.</p>
</div>
<div id="akaike-information-criterion-aic" class="section level3">
<h3>Akaike Information Criterion (AIC)</h3>
<p>It assesses how well a model fits the data it generates.The
calculation of the AIC relies on the number of independent variables
used to build the model and the maximum likelihood estimate of the
model. According to AIC, the best-fitting model is the one that explains
the greatest amount of variation with the fewest independent variables,
whichFewer variables will also enhance the optimization of the model to
reduce error. Each redundant predictor added to the model will introduce
a penalty.</p>
</div>
<div id="area-under-the-roc-curve-auc" class="section level3">
<h3>Area under the ROC Curve (AUC)</h3>
<p>An ROC curve is a graph showing the performance of a classification
model at all classification thresholds. AUC measures the entire
two-dimensional area underneath the entire ROC curve. The higher the AUC
score, the better the classification of the predicted values is. For
example, if we consider a model to predict and classify whether the
outcome is ‘Diabetes’’ or ‘Normal’, a high AUC indicates that the model
is capable of classifying ‘Diabetes” as ’Diabetes” and ‘Normal’ as
‘Normal’ more efficiently. There are two reasons why we compare AUC. On
one hand, AUC is scale-invariant. It measures the ranking of predictions
rather than their absolute values. On the other hand, AUC measures the
quality of the model’s predictions irrespective of what classification
threshold is chosen. Full model and subset model’s AUC is slightly
higher than the stepwise model but not with significant statistical
value.</p>
</div>
<div id="prediction-accuracy" class="section level3">
<h3>Prediction accuracy</h3>
<p>Prediction accuracy was calculated by predicting the testing data
using the model fitted from the training data. If the predicted value is
less than 0.5, we classified it as 0. If the predicted value is larger
than 0.5, we classified it as 1. Then we computed the ratio of predicted
value from the test data to the observed value of testing data to see
how much percentage of the predicted value matched to the observed value
and recorded it as prediction accuracy.</p>
</div>
<div id="conclusion" class="section level3">
<h3>Conclusion</h3>
<p>From the comparison table, we see that it returns similar values
among RMSE of 0.42, MAE of 0.35, prediction accuracy of 0.74. Stepwise
regression’s AIC and AUC is slightly lower than the full model’s and
subset model’s respectively but the difference is not statistically
significant.</p>
<p>Therefore, we select the stepwise regression model with the fewest
predictors following the principle of parsimony.</p>
</div>
<div id="why-use-aic-instead-of-bic" class="section level3">
<h3>Why use AIC instead of BIC?</h3>
<p>The reason we use AIC is because Bayesian information criterion (BIC)
usually results in more parsimonious model than the Akaike information
criterion.</p>
</div>
</div>
<div id="model-interpretation" class="section level2">
<h2>Model interpretation</h2>
<pre class="r"><code>variable_name &lt;- c(&quot;Pregnancies&quot;, &quot;Glucose&quot;, &quot;Blood Pressure&quot;, &quot;BMI&quot;, &quot;Diabetes Pedigree Function&quot;,&quot;Age&quot;)
val &lt;- c(1.021,1.006,0.998,1.013,1.151,1.003)
p &lt;- c(&quot;&lt; 0.05&quot;,&quot;&lt; 0.05&quot;,&quot;&lt; 0.05&quot;,&quot;&lt; 0.05&quot;,&quot;&lt; 0.05&quot;,0.069)
variable_data &lt;- data.frame(variable_name,val,p)
kable(variable_data,caption = &quot;Table 2.Parameter estimates for the logistic model&quot;, col.names = c(&quot;Parameters&quot;,&quot;Exponentiated Coefficients/Odds Ratio&quot;,&quot;P-values&quot;))%&gt;% kable_styling(latex_option = c(&quot;hold_position&quot;), position = &quot;left&quot;)</code></pre>
<table class="table" style>
<caption>
Table 2.Parameter estimates for the logistic model
</caption>
<thead>
<tr>
<th style="text-align:left;">
Parameters
</th>
<th style="text-align:right;">
Exponentiated Coefficients/Odds Ratio
</th>
<th style="text-align:left;">
P-values
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Pregnancies
</td>
<td style="text-align:right;">
1.021
</td>
<td style="text-align:left;">
&lt; 0.05
</td>
</tr>
<tr>
<td style="text-align:left;">
Glucose
</td>
<td style="text-align:right;">
1.006
</td>
<td style="text-align:left;">
&lt; 0.05
</td>
</tr>
<tr>
<td style="text-align:left;">
Blood Pressure
</td>
<td style="text-align:right;">
0.998
</td>
<td style="text-align:left;">
&lt; 0.05
</td>
</tr>
<tr>
<td style="text-align:left;">
BMI
</td>
<td style="text-align:right;">
1.013
</td>
<td style="text-align:left;">
&lt; 0.05
</td>
</tr>
<tr>
<td style="text-align:left;">
Diabetes Pedigree Function
</td>
<td style="text-align:right;">
1.151
</td>
<td style="text-align:left;">
&lt; 0.05
</td>
</tr>
<tr>
<td style="text-align:left;">
Age
</td>
<td style="text-align:right;">
1.003
</td>
<td style="text-align:left;">
0.069
</td>
</tr>
</tbody>
</table>
<p>The final best model generated is as following: <span
class="math inline">\((pi/1-pi) = 0.443 +
1.021(Pregnancies)+1.006(Glucose)+0.998(BloodPressure)+1.013(BMI)+1.151(DiabetesPedigreeFunction)+1.003(Age)\)</span></p>
<p>pi is the probability for diabetes</p>
<ul>
<li><p>On average, for one more pregnancy, the estimated increase in
diabetes odds is 1.021, holding other variables constant.</p></li>
<li><p>For one unit increase in glucose, the estimated increase in
diabetes odds on average is 1.006, holding other variables
constant.</p></li>
<li><p>For one mmHg increase in blood pressure, the estimated increase
in diabetes odds on average is 0.998, holding other variables
constant.</p></li>
<li><p>For one unit increase in BMI, the estimated increase in diabetes
odds on average is 1.013, holding other variables constant.</p></li>
<li><p>For one unit increase in diabetes pedigree function, the
estimated increase in diabetes odds on average is 1.151, , holding other
variables constant.</p></li>
<li><p>For one year increase in age, the estimated increase in diabetes
odds on average is around 1.003, holding other variables
constant.</p></li>
</ul>
<p>The six predictor estimators were listed in the table, and four of
them have a positive association with diabetes. However, we discovered
that the blood pressure and Glucose variables have the estimated odds
around one, which implies no relationship with diabetes. What we expect
is that the Glucose and blood pressure variable will help us predict the
disease of diabetes. One possible explanation is that units of Glucose
and blood pressure variables are very small. A diminutive unit increase
of these two variables results in only a slight change in the outcome of
p/1-pi. Therefore, the coefficient will be close to 1 as the overall
change is little. The small increase is caused by the absolute change in
the values instead of the small influence of glucose and blood pressure
on diabetes.</p>
<p>Glucose and blood pressure will be at their normal range if people do
not have disease, and the odds result of these two only represent the
rate of outcome as unit change as we considered the unit of the glucose
and blood pressure is small. Also, the measure method and difference in
measurement time will affect the glucose and blood pressure result. That
might be the reason why the odds of glucose and blood pressure are
around one.</p>
</div>
<div id="assumptions-check" class="section level2">
<h2>Assumptions Check</h2>
<p>We performed assumptions checks on our model.</p>
<div id="assumption-1-appropriate-outcome-type" class="section level3">
<h3>Assumption 1— Appropriate Outcome Type</h3>
<p>Our goal is to establish a model to predict the occurrence of
diabetes where the outcome is “Diabetes” or “Normal”. By default,
logistic regression assumes that the outcome variable is binary, meeting
our criteria.</p>
</div>
<div id="assumption-2-linearity-of-independent-variables-and-log-odds"
class="section level3">
<h3>Assumption 2 — Linearity of independent variables and log-odds</h3>
<pre class="r"><code>crPlots(stepmodel)</code></pre>
<p><img src="StatisticaL-Analysis_files/figure-html/unnamed-chunk-17-1.png" width="624" style="display: block; margin: auto;" /></p>
<p>We can check logit linearity by visually inspecting the scatter plot
between each predictor and the logit values.</p>
<p>Component residual plot is an extension of partial residual plots,
which is desirable to check whether predictors have a linear
relationship with dependent variables. A partial residual plot
essentially attempts to model the residuals of one predictor against the
dependent variable. A component residual plot adds a line indicating
where the line of best fit lies. A significant difference between the
residual line and the component line indicates that the predictor does
not have a linear relationship with the dependent variable.</p>
<p>The blue line shows the expected residuals if the relationship
between the predictor and response variable was linear. The pink line
shows the actual residuals. If the blue line and pink line basically
overlaps, we can conclude that there is a linear relationship between
the outcome and predictor.</p>
<p>From component residual plots of each variable, we see that all
predictors except glucose display a prominent linear relationship with
outcome. Tails of the pink line in the glucose graph deviated a little
from the blue line but we can still conclude it linearwise.</p>
</div>
<div id="assumption-3-no-strongly-influential-outliers"
class="section level3">
<h3>Assumption 3— No strongly influential outliers</h3>
<p>Influential values are extreme individual data points that can alter
the quality of the logistic regression model. We used Cook’s Distance to
determine the influence of a data point. Cook’s difference is calculated
based on residual and leverage of each data point. It summarizes the
changes in the regression model when that particular (ith ) observation
is removed.</p>
<p>The most extreme values in the data can be examined by visualizing
the Cook’s distance values. Here we label the top 3 largest values: Note
that not all outliers are influential observations. To check whether the
data contains potential outliers, the standardized residual error can be
inspected. Data points with absolute standardized residuals above 3
represent possible outliers and may deserve closer attention to
determine its influence.</p>
<pre class="r"><code>plot(stepmodel, which = 4, id.n = 3)</code></pre>
<p><img src="StatisticaL-Analysis_files/figure-html/unnamed-chunk-18-1.png" width="528" style="display: block; margin: auto;" /></p>
</div>
<div id="assumption-4-absence-of-multicollinearity"
class="section level3">
<h3>Assumption 4 — Absence of Multicollinearity</h3>
<p>Multicollinearity corresponds to a situation where the data contain
highly correlated independent variables. With highly correlated
variables, the standard error will be inflated and R-square value will
falsely increase. This is a problem because it reduces the precision of
the estimated coefficients, which weakens the statistical power of the
logistic regression model.</p>
<p>Variance Inflation Factor (VIF) measures the degree of
multicollinearity in a set of independent variables. Mathematically, it
is equal to the ratio of the overall model variance to the variance of a
model that includes only that single independent variable.VIF&gt;5
indicates a cause for concern of multicollinearity while VIF &gt; 10
indicates a serious collinearity problem in the model.</p>
<p>In our model, all predictors have VIF between 1 and 1.6, which is
closer to the smallest possible for VIF of 1. Our model is free form
collinearity.</p>
<pre class="r"><code>all_vifs &lt;- car::vif(stepmodel)</code></pre>
<pre class="r"><code>print(all_vifs) %&gt;% tidy()%&gt;%
  mutate(Variables = names,
         VIF = x) %&gt;%
  dplyr::select(Variables,VIF)%&gt;%
  knitr::kable(digits = 3,caption = &quot;Table 3.VIF of all variables&quot;)%&gt;%
  kable_styling(latex_option = c(&quot;hold_position&quot;), position = &quot;left&quot;)</code></pre>
<pre><code>##              Pregnancies                  Glucose            BloodPressure 
##                 1.427047                 1.143559                 1.154015 
##                      BMI DiabetesPedigreeFunction                      Age 
##                 1.148072                 1.036558                 1.555786</code></pre>
<table class="table" style>
<caption>
Table 3.VIF of all variables
</caption>
<thead>
<tr>
<th style="text-align:left;">
Variables
</th>
<th style="text-align:right;">
VIF
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Pregnancies
</td>
<td style="text-align:right;">
1.427
</td>
</tr>
<tr>
<td style="text-align:left;">
Glucose
</td>
<td style="text-align:right;">
1.144
</td>
</tr>
<tr>
<td style="text-align:left;">
BloodPressure
</td>
<td style="text-align:right;">
1.154
</td>
</tr>
<tr>
<td style="text-align:left;">
BMI
</td>
<td style="text-align:right;">
1.148
</td>
</tr>
<tr>
<td style="text-align:left;">
DiabetesPedigreeFunction
</td>
<td style="text-align:right;">
1.037
</td>
</tr>
<tr>
<td style="text-align:left;">
Age
</td>
<td style="text-align:right;">
1.556
</td>
</tr>
</tbody>
</table>
</div>
<div id="assumption-5-independence-of-observations"
class="section level3">
<h3>Assumption 5— Independence of observations</h3>
<p>In logistic regression, it assumes that observations are independent
of each other, i.e., they should not come from repeated or paired data.
This means that each observation is not influenced by or related to the
rest of the observations.</p>
<p>This independence assumption is automatically met for our dataset
since the data consists of individual records.</p>
<p>We can also check this assumption by creating a Residual Series plot
which plots the deviance residuals of the logit model against the index
numbers of the observations. Since the residuals in the plot above
appear to be randomly scattered around the centerline of zero, we can
infer visually that the assumption is satisfied.</p>
<pre class="r"><code>model.data &lt;- augment(stepmodel) %&gt;% 
  mutate(index = 1:n()) 
ggplot(model.data, aes(index, .std.resid)) + 
  geom_point(aes(color = factor(Outcome)), alpha = .5) +
  theme_bw()+
  labs(title=&quot;Residual Series Plot&quot;,y=&quot;Deviance Residuals&quot;,color=&quot;Outcome&quot;)+
  theme(plot.title = element_text(color = &quot;darkgrey&quot;,size = 13))+
  scale_color_manual(labels=c(&quot;Normal&quot;,&quot;Diabetes&quot;),values=c(&quot;blue&quot;,&quot;red&quot;))</code></pre>
<p><img src="StatisticaL-Analysis_files/figure-html/unnamed-chunk-21-1.png" width="576" style="display: block; margin: auto;" /></p>
</div>
<div id="assumption-6-sufficiently-large-sample-size"
class="section level3">
<h3>Assumption 6 — Sufficiently large sample size</h3>
<p>There should be an adequate number of observations for each
independent variable in the dataset to avoid creating an overfit
model.</p>
<p>A common way to determine a large sample size is that the total
number of observations should be greater than 500. Our data set contains
768 observations, which is sufficiently large.</p>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->
<script>
$(document).ready(function () {
  window.initializeCodeFolding("hide" === "show");
});
</script>

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
